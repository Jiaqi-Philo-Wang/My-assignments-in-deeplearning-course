{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "电商18 180412126 王佳琦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业参考：https://www.bbsmax.com/A/n2d9MB7gdD/ 和https://blog.csdn.net/winycg/article/details/90318371"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入手写数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, cuda=False, epochs=10, log_interval=10, no_cuda=False, seed=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example') #创建一个 ArgumentParser 对象。ArgumentParser 对象包含将命令行解析成 Python 数据类型所需的全部信息。\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')   #添加程序参数信息\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args(args=[])  #这里进行了修改，如果不修改，会报错。\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(args)   #参数\n",
    " \n",
    "#Sets the seed for generating random numbers. And returns a torch._C.Generator object.\n",
    "torch.manual_seed(args.seed)   #随机数\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "trainset = datasets.MNIST('/data', train=True, download=True,transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([1, 28, 28])\n",
      "10000\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "testset= datasets.MNIST('/data', train=False, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "image, label = trainset[0]\n",
    "print(len(trainset))\n",
    "print(image.size())\n",
    "image, label = testset[0]\n",
    "print(len(testset))\n",
    "print(image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    " \n",
    "        self.fc1 = nn.Linear(784, 400)         #全连接层\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    " \n",
    "        self.relu = nn.ReLU()                    #激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def encode(self, x):                          #编码\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    " \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = Variable(std.data.new(std.size()).normal_())\n",
    "        return eps.mul(std).add_(mu)\n",
    " \n",
    "    def decode(self, z):                        #解码\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    " \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    " \n",
    "model = VAE()\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    " \n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x.view(-1, 784))\n",
    " \n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    " \n",
    "    return BCE + KLD\n",
    " \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss #loss_sum += loss.data[0]\n",
    "\n",
    "        #这是因为输出的loss的数据类型是Variable。而PyTorch的动态图机制就是通过Variable来构建图。\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss/ len(data)))\n",
    " \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    fake_images = gen_imgs.view(-1, 1, 28, 28)\n",
    "    save_image(fake_images, 'MNIST_FAKE/fake_images-{}.png'.format(epoch + 1))\n",
    " \n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar)\n",
    " \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.002090\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.002095\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.002109\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.002062\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.002089\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.002034\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.002111\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.002040\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.002064\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.002029\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.002135\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.002087\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.002062\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.002107\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.002132\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.002060\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.002059\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.002189\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.002022\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.002023\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.002065\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.002066\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.002042\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.002054\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.002032\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.002090\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.001986\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.002053\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.002126\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.002070\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.002053\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.002099\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.002104\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.002035\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.002008\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.002032\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.002083\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.002078\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.002075\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.002084\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.002057\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.002007\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.002046\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.002095\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.002015\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.002091\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.002170\n",
      "====> Epoch: 1 Average loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-6fba016bd045>:31: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data = Variable(data, volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.002119\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.002061\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.002109\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.002052\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.002046\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.002032\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.002019\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.001993\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.002141\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.002094\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.002062\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.002117\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.002025\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.002057\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.002044\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001980\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.002066\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.002073\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.002039\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.002051\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.002021\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.002067\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.002092\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.002049\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.002105\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.002104\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.002096\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.002098\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.002083\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.001976\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.002029\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.002023\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.001985\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.002053\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.002080\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.002053\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.002047\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.001997\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.002098\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.001963\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.002067\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.002121\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.002088\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.002038\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.002028\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.002096\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.002106\n",
      "====> Epoch: 2 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001986\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.002031\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.002089\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.002062\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.002145\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001991\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.002058\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.002055\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.002060\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.002067\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.002011\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.002132\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.002130\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.002046\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.002039\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.002052\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.002141\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.002076\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.002107\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.002104\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.002068\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.002033\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.002078\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.002092\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.001938\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.002120\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.002075\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.002067\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.002030\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.002085\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001990\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.002037\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.002041\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.002051\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.002017\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.002077\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.002063\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.002031\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.002064\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.002083\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.002055\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.002031\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.002011\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.002091\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.002127\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.002056\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.002066\n",
      "====> Epoch: 3 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.002072\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.002090\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.002034\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.002021\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.002095\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.002113\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.002030\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.002037\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.002009\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.002078\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.002070\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.002103\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.002068\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.002089\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.002093\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.002014\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.002001\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.002032\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.002064\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.002076\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.002100\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.002148\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.002068\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.002079\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.002044\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.002042\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.002119\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.002099\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.002011\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.002121\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.002108\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.002040\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.001951\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.002061\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.001994\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.002142\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.002117\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.002104\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.002109\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.002073\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001999\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.002072\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.002037\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.002088\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.002085\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.002078\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.002038\n",
      "====> Epoch: 4 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.002064\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.001940\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.002066\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.002063\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.002029\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.002050\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.002071\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.002113\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.002040\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.002029\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.002067\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.002170\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.002109\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.002009\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.002083\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.002137\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.001983\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.002071\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.002085\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.002142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002103\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.002031\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.002049\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.002070\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.002014\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.002037\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.002061\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.002065\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.002049\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.002040\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.002068\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.002111\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.002045\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.002073\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.002130\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.002047\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.002007\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.002123\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.002069\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.002079\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001996\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.001992\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.002066\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.002042\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.002043\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.002041\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.002061\n",
      "====> Epoch: 5 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.002142\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.002083\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.002046\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.002132\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.002055\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.002050\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.002034\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.002023\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.002063\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.002079\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.002016\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.002104\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.002077\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.002161\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.002009\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.002011\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.002130\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.002156\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.002096\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.001984\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.002044\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.002135\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.002090\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.002030\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.002001\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.002031\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.002101\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.002102\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.002024\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.002059\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.001985\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.002083\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.001998\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.002081\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.002041\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002112\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.001996\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.002049\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.002108\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.002042\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.001957\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.002028\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.002037\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.002159\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.002115\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002014\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.002076\n",
      "====> Epoch: 6 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002059\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.002053\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.002011\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.002107\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.002055\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.002043\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.002052\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.002005\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.002015\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.002007\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.002037\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.002059\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.001999\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.002034\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.001971\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.002018\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.002054\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.002077\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.002006\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.002106\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.002085\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.002019\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.002079\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.002077\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.002032\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.002007\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.002108\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.002049\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.002104\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.002058\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.002084\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.002098\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.002046\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.002027\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.002040\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.002043\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.002074\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.002069\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.002073\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.002031\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.002121\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.002056\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.002074\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.002038\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.002065\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.002006\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.002043\n",
      "====> Epoch: 7 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.002068\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.002102\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.002045\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.002055\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.002012\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002050\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.002028\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.002018\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.002065\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.002068\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.002162\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.002075\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.002040\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.002079\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.002005\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.002070\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.002026\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.002094\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.002033\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.002027\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002119\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.002032\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.001974\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.002104\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.002028\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.002009\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.002043\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.002109\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.002023\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.002006\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.002052\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.002098\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.002098\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.002054\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.002000\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.002106\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.002008\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.002029\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.002134\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.002037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.001973\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.001995\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.002081\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.002054\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.002143\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.002004\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.001975\n",
      "====> Epoch: 8 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002074\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.002071\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.001993\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.002116\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.001995\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.002034\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.002004\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.002047\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.002073\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.002078\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.002065\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.001993\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.002031\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.002111\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.002049\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.002078\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.002055\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.002101\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.002045\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.002017\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.002073\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.002010\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.002029\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.002081\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.002034\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.001987\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.002126\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.002067\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.002021\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.002021\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.002064\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.002089\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.002004\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.001989\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002058\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.002002\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.002074\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.002081\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.002071\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002033\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.002046\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.002025\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.002054\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.001983\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.002019\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.002021\n",
      "====> Epoch: 9 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001997\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.002071\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.002043\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.002071\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.002096\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.002021\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.002021\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.002011\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.001988\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.002056\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002084\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.002047\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.002080\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.002106\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.002063\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002026\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.002040\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.002098\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.002079\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.002107\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001994\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.002045\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.002031\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.002099\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.002034\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002042\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.002092\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.002145\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.002054\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.002063\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002077\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.002070\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.001973\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.002083\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.002075\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002111\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.001998\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.002033\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.002032\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.002055\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002076\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.002101\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.002053\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.002094\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.002044\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.002067\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.002015\n",
      "====> Epoch: 10 Average loss: 0.0021\n",
      "====> Test set loss: 0.0021\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 输出生成图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个新的文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainset\n",
    " \n",
    "# 数据加载器\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim) # 均值 向量\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim) # 保准方差 向量\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    # 编码过程\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    # 随机生成隐含向量\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    " \n",
    "    # 解码过程\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    \n",
    "    # 整个前向传播过程：编码-》解码\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/15], Step [100/469], Reconst Loss: 23024.3047, KL Div: 1279.8553\n",
      "Epoch[1/15], Step [200/469], Reconst Loss: 17445.0176, KL Div: 2054.7214\n",
      "Epoch[1/15], Step [300/469], Reconst Loss: 14989.7158, KL Div: 2397.6909\n",
      "Epoch[1/15], Step [400/469], Reconst Loss: 14289.7920, KL Div: 2552.9219\n",
      "Epoch[2/15], Step [100/469], Reconst Loss: 12673.1621, KL Div: 2745.3796\n",
      "Epoch[2/15], Step [200/469], Reconst Loss: 12700.6367, KL Div: 2967.8745\n",
      "Epoch[2/15], Step [300/469], Reconst Loss: 11851.6396, KL Div: 3136.5991\n",
      "Epoch[2/15], Step [400/469], Reconst Loss: 12564.5088, KL Div: 3087.8352\n",
      "Epoch[3/15], Step [100/469], Reconst Loss: 12041.7598, KL Div: 3142.9043\n",
      "Epoch[3/15], Step [200/469], Reconst Loss: 11142.5586, KL Div: 3031.6726\n",
      "Epoch[3/15], Step [300/469], Reconst Loss: 11204.2998, KL Div: 3187.9622\n",
      "Epoch[3/15], Step [400/469], Reconst Loss: 11661.8887, KL Div: 3112.5471\n",
      "Epoch[4/15], Step [100/469], Reconst Loss: 11244.3086, KL Div: 3212.1965\n",
      "Epoch[4/15], Step [200/469], Reconst Loss: 11115.0625, KL Div: 3099.9084\n",
      "Epoch[4/15], Step [300/469], Reconst Loss: 11189.0352, KL Div: 3200.8025\n",
      "Epoch[4/15], Step [400/469], Reconst Loss: 11102.6230, KL Div: 3179.8350\n",
      "Epoch[5/15], Step [100/469], Reconst Loss: 10640.2344, KL Div: 3128.2139\n",
      "Epoch[5/15], Step [200/469], Reconst Loss: 11135.2686, KL Div: 3181.0244\n",
      "Epoch[5/15], Step [300/469], Reconst Loss: 11180.4258, KL Div: 3258.2632\n",
      "Epoch[5/15], Step [400/469], Reconst Loss: 11046.9980, KL Div: 3332.0745\n",
      "Epoch[6/15], Step [100/469], Reconst Loss: 10421.2637, KL Div: 3162.3691\n",
      "Epoch[6/15], Step [200/469], Reconst Loss: 10545.3945, KL Div: 3350.5222\n",
      "Epoch[6/15], Step [300/469], Reconst Loss: 10733.3730, KL Div: 3302.6091\n",
      "Epoch[6/15], Step [400/469], Reconst Loss: 10766.5039, KL Div: 3318.1509\n",
      "Epoch[7/15], Step [100/469], Reconst Loss: 10424.6523, KL Div: 3285.3882\n",
      "Epoch[7/15], Step [200/469], Reconst Loss: 10493.9775, KL Div: 3175.3945\n",
      "Epoch[7/15], Step [300/469], Reconst Loss: 10804.8125, KL Div: 3362.6790\n",
      "Epoch[7/15], Step [400/469], Reconst Loss: 10819.4277, KL Div: 3240.5464\n",
      "Epoch[8/15], Step [100/469], Reconst Loss: 10309.3320, KL Div: 3278.4277\n",
      "Epoch[8/15], Step [200/469], Reconst Loss: 10224.3711, KL Div: 3199.2783\n",
      "Epoch[8/15], Step [300/469], Reconst Loss: 10722.4805, KL Div: 3376.5215\n",
      "Epoch[8/15], Step [400/469], Reconst Loss: 10116.1631, KL Div: 3222.5881\n",
      "Epoch[9/15], Step [100/469], Reconst Loss: 10217.0742, KL Div: 3294.9424\n",
      "Epoch[9/15], Step [200/469], Reconst Loss: 10342.9219, KL Div: 3272.9878\n",
      "Epoch[9/15], Step [300/469], Reconst Loss: 10684.4912, KL Div: 3234.7676\n",
      "Epoch[9/15], Step [400/469], Reconst Loss: 10114.1484, KL Div: 3226.2727\n",
      "Epoch[10/15], Step [100/469], Reconst Loss: 10612.8027, KL Div: 3352.6987\n",
      "Epoch[10/15], Step [200/469], Reconst Loss: 10334.0195, KL Div: 3318.3115\n",
      "Epoch[10/15], Step [300/469], Reconst Loss: 9951.4512, KL Div: 3124.9243\n",
      "Epoch[10/15], Step [400/469], Reconst Loss: 10358.7754, KL Div: 3253.4099\n",
      "Epoch[11/15], Step [100/469], Reconst Loss: 10329.7480, KL Div: 3281.6934\n",
      "Epoch[11/15], Step [200/469], Reconst Loss: 10194.5547, KL Div: 3313.7456\n",
      "Epoch[11/15], Step [300/469], Reconst Loss: 9871.5146, KL Div: 3278.8169\n",
      "Epoch[11/15], Step [400/469], Reconst Loss: 9979.6455, KL Div: 3301.8396\n",
      "Epoch[12/15], Step [100/469], Reconst Loss: 10609.5215, KL Div: 3236.4607\n",
      "Epoch[12/15], Step [200/469], Reconst Loss: 10037.7598, KL Div: 3329.5864\n",
      "Epoch[12/15], Step [300/469], Reconst Loss: 10217.7520, KL Div: 3291.2266\n",
      "Epoch[12/15], Step [400/469], Reconst Loss: 10331.7422, KL Div: 3274.6802\n",
      "Epoch[13/15], Step [100/469], Reconst Loss: 9953.2227, KL Div: 3230.6482\n",
      "Epoch[13/15], Step [200/469], Reconst Loss: 10429.3086, KL Div: 3201.5608\n",
      "Epoch[13/15], Step [300/469], Reconst Loss: 10435.9258, KL Div: 3145.6389\n",
      "Epoch[13/15], Step [400/469], Reconst Loss: 10207.6465, KL Div: 3185.2925\n",
      "Epoch[14/15], Step [100/469], Reconst Loss: 10163.1465, KL Div: 3282.1899\n",
      "Epoch[14/15], Step [200/469], Reconst Loss: 10176.8291, KL Div: 3301.1987\n",
      "Epoch[14/15], Step [300/469], Reconst Loss: 9875.9219, KL Div: 3232.0862\n",
      "Epoch[14/15], Step [400/469], Reconst Loss: 10530.4668, KL Div: 3473.6743\n",
      "Epoch[15/15], Step [100/469], Reconst Loss: 10006.4170, KL Div: 3328.2034\n",
      "Epoch[15/15], Step [200/469], Reconst Loss: 10131.5645, KL Div: 3221.3340\n",
      "Epoch[15/15], Step [300/469], Reconst Loss: 10323.8975, KL Div: 3206.9609\n",
      "Epoch[15/15], Step [400/469], Reconst Loss: 9832.2910, KL Div: 3177.8984\n"
     ]
    }
   ],
   "source": [
    "# 实例化一个模型\n",
    "model = VAE().to(device)\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # 获取样本，并前向传播\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        # 计算重构损失和KL散度（KL散度用于衡量两种分布的相似程度）\n",
    "        # KL散度的计算可以参考论文或者文章开头的链接\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    \n",
    "    # 利用训练的模型进行测试\n",
    "    with torch.no_grad():\n",
    "        # 随机生成的图像\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 28, 28) # 第一维数据不变，后一维数据转化为(1, 28, 28)\n",
    "        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "        # 重构的图像\n",
    "        out, _, _ = model(x)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 参考上列代码做法处理cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])#数据集加载时，默认的图片格式是 numpy，所以通过 transforms 转换成 Tensor。\n",
    "                                                              #然后，再对输入图片进行标准化。\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'samples-cifar'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainset\n",
    " \n",
    "# 数据加载器\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=12288, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim) # 均值 向量\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim) # 保准方差 向量\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    # 编码过程\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    # 随机生成隐含向量\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    " \n",
    "    # 解码过程\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    \n",
    "    # 整个前向传播过程：编码-》解码\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 12288\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "model = VAE().to(device)\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goingon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "D:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/15], Step [100/12500], Reconst Loss: -52152.1406, KL Div: 1473.6368\n",
      "Epoch[1/15], Step [200/12500], Reconst Loss: -190529.2969, KL Div: 4438.0088\n",
      "Epoch[1/15], Step [300/12500], Reconst Loss: -42189.6016, KL Div: 1713.4611\n",
      "Epoch[1/15], Step [400/12500], Reconst Loss: 6117.5293, KL Div: 1089.2856\n",
      "Epoch[1/15], Step [500/12500], Reconst Loss: 218.0721, KL Div: 1142.3660\n",
      "Epoch[1/15], Step [600/12500], Reconst Loss: -21092.1738, KL Div: 1548.6235\n",
      "Epoch[1/15], Step [700/12500], Reconst Loss: -322608.9375, KL Div: 2766.1714\n",
      "Epoch[1/15], Step [800/12500], Reconst Loss: 389.6350, KL Div: 378.6440\n",
      "Epoch[1/15], Step [900/12500], Reconst Loss: -147971.6250, KL Div: 1097.3308\n",
      "Epoch[1/15], Step [1000/12500], Reconst Loss: -7457.0908, KL Div: 481.3492\n",
      "Epoch[1/15], Step [1100/12500], Reconst Loss: -107349.7188, KL Div: 875.4446\n",
      "Epoch[1/15], Step [1200/12500], Reconst Loss: 4632.7217, KL Div: 1872.7836\n",
      "Epoch[1/15], Step [1300/12500], Reconst Loss: -41779.0742, KL Div: 1027.6180\n",
      "Epoch[1/15], Step [1400/12500], Reconst Loss: -6291.7646, KL Div: 1190.4976\n",
      "Epoch[1/15], Step [1500/12500], Reconst Loss: -137584.1250, KL Div: 2103.2012\n",
      "Epoch[1/15], Step [1600/12500], Reconst Loss: -107886.9766, KL Div: 650.0766\n",
      "Epoch[1/15], Step [1700/12500], Reconst Loss: -55520.5547, KL Div: 1351.5131\n",
      "Epoch[1/15], Step [1800/12500], Reconst Loss: -43824.2266, KL Div: 666.3855\n",
      "Epoch[1/15], Step [1900/12500], Reconst Loss: -89571.5000, KL Div: 811.1243\n",
      "Epoch[1/15], Step [2000/12500], Reconst Loss: -12729.7129, KL Div: 2985.5886\n",
      "Epoch[1/15], Step [2100/12500], Reconst Loss: -18768.8340, KL Div: 5563.6411\n",
      "Epoch[1/15], Step [2200/12500], Reconst Loss: -59954.3008, KL Div: 1307.9171\n",
      "Epoch[1/15], Step [2300/12500], Reconst Loss: -12498.1270, KL Div: 913.3545\n",
      "Epoch[1/15], Step [2400/12500], Reconst Loss: -248685.3125, KL Div: 2304.7375\n",
      "Epoch[1/15], Step [2500/12500], Reconst Loss: -93089.4219, KL Div: 1813.8876\n",
      "Epoch[1/15], Step [2600/12500], Reconst Loss: -4289.4341, KL Div: 2900.0981\n",
      "Epoch[1/15], Step [2700/12500], Reconst Loss: -179121.7812, KL Div: 1788.2510\n",
      "Epoch[1/15], Step [2800/12500], Reconst Loss: -66118.9141, KL Div: 1006.2148\n",
      "Epoch[1/15], Step [2900/12500], Reconst Loss: -92408.1719, KL Div: 2616.9641\n",
      "Epoch[1/15], Step [3000/12500], Reconst Loss: -86177.7734, KL Div: 2456.2656\n",
      "Epoch[1/15], Step [3100/12500], Reconst Loss: -167936.1406, KL Div: 1530.0884\n",
      "Epoch[1/15], Step [3200/12500], Reconst Loss: -108876.2812, KL Div: 1075.1097\n",
      "Epoch[1/15], Step [3300/12500], Reconst Loss: -170051.8594, KL Div: 1817.0681\n",
      "Epoch[1/15], Step [3400/12500], Reconst Loss: -149568.6562, KL Div: 1168.6177\n",
      "Epoch[1/15], Step [3500/12500], Reconst Loss: -80816.3438, KL Div: 1092.2069\n",
      "Epoch[1/15], Step [3600/12500], Reconst Loss: -30534.9531, KL Div: 1821.7140\n",
      "Epoch[1/15], Step [3700/12500], Reconst Loss: -34692.5898, KL Div: 2271.6626\n",
      "Epoch[1/15], Step [3800/12500], Reconst Loss: -149888.7500, KL Div: 3016.9004\n",
      "Epoch[1/15], Step [3900/12500], Reconst Loss: -98843.0547, KL Div: 2936.2156\n",
      "Epoch[1/15], Step [4000/12500], Reconst Loss: 12193.1768, KL Div: 10238.3242\n",
      "Epoch[1/15], Step [4100/12500], Reconst Loss: -194007.2031, KL Div: 4693.0757\n",
      "Epoch[1/15], Step [4200/12500], Reconst Loss: -207428.4844, KL Div: 7802.8125\n",
      "Epoch[1/15], Step [4300/12500], Reconst Loss: -90780.8125, KL Div: 4763.3745\n",
      "Epoch[1/15], Step [4400/12500], Reconst Loss: -110994.9844, KL Div: 5576.7227\n",
      "Epoch[1/15], Step [4500/12500], Reconst Loss: -164102.4062, KL Div: 6209.4014\n",
      "Epoch[1/15], Step [4600/12500], Reconst Loss: -87135.1016, KL Div: 2002.6464\n",
      "Epoch[1/15], Step [4700/12500], Reconst Loss: -28004.1055, KL Div: 4482.1851\n",
      "Epoch[1/15], Step [4800/12500], Reconst Loss: 2778.9136, KL Div: 5802.6802\n",
      "Epoch[1/15], Step [4900/12500], Reconst Loss: -44027.6875, KL Div: 5459.2964\n",
      "Epoch[1/15], Step [5000/12500], Reconst Loss: -7508.1025, KL Div: 2596.7651\n",
      "Epoch[1/15], Step [5100/12500], Reconst Loss: -56207.1797, KL Div: 4138.9629\n",
      "Epoch[1/15], Step [5200/12500], Reconst Loss: -39128.2500, KL Div: 3378.4585\n",
      "Epoch[1/15], Step [5300/12500], Reconst Loss: -13537.4502, KL Div: 5276.6997\n",
      "Epoch[1/15], Step [5400/12500], Reconst Loss: -126444.6875, KL Div: 10935.3896\n",
      "Epoch[1/15], Step [5500/12500], Reconst Loss: -164164.4062, KL Div: 5297.2881\n",
      "Epoch[1/15], Step [5600/12500], Reconst Loss: -158815.8438, KL Div: 4871.9131\n",
      "Epoch[1/15], Step [5700/12500], Reconst Loss: -305013.4062, KL Div: 8186.9951\n",
      "Epoch[1/15], Step [5800/12500], Reconst Loss: -238266.4375, KL Div: 13950.0645\n",
      "Epoch[1/15], Step [5900/12500], Reconst Loss: -26948.2969, KL Div: 2145.2505\n",
      "Epoch[1/15], Step [6000/12500], Reconst Loss: -251944.8438, KL Div: 5875.0444\n",
      "Epoch[1/15], Step [6100/12500], Reconst Loss: -26917.9238, KL Div: 5080.7964\n",
      "Epoch[1/15], Step [6200/12500], Reconst Loss: -39232.9414, KL Div: 2128.1045\n",
      "Epoch[1/15], Step [6300/12500], Reconst Loss: -28141.2363, KL Div: 3742.4348\n",
      "Epoch[1/15], Step [6400/12500], Reconst Loss: 531.8039, KL Div: 3935.2261\n",
      "Epoch[1/15], Step [6500/12500], Reconst Loss: -145579.8750, KL Div: 3201.6614\n",
      "Epoch[1/15], Step [6600/12500], Reconst Loss: -175366.5625, KL Div: 12688.8467\n",
      "Epoch[1/15], Step [6700/12500], Reconst Loss: -20419.4082, KL Div: 6700.4526\n",
      "Epoch[1/15], Step [6800/12500], Reconst Loss: -74078.0938, KL Div: 2381.3232\n",
      "Epoch[1/15], Step [6900/12500], Reconst Loss: -36278.0312, KL Div: 6008.7378\n",
      "Epoch[1/15], Step [7000/12500], Reconst Loss: -302711.4375, KL Div: 4762.8120\n",
      "Epoch[1/15], Step [7100/12500], Reconst Loss: -254190.4844, KL Div: 4291.4761\n",
      "Epoch[1/15], Step [7200/12500], Reconst Loss: -196184.9688, KL Div: 3651.3640\n",
      "Epoch[1/15], Step [7300/12500], Reconst Loss: -129943.0312, KL Div: 6198.4580\n",
      "Epoch[1/15], Step [7400/12500], Reconst Loss: -57100.2773, KL Div: 3245.7483\n",
      "Epoch[1/15], Step [7500/12500], Reconst Loss: -260070.5625, KL Div: 5409.3652\n",
      "Epoch[1/15], Step [7600/12500], Reconst Loss: -129894.9531, KL Div: 6373.9600\n",
      "Epoch[1/15], Step [7700/12500], Reconst Loss: -18667.9102, KL Div: 14105.6406\n",
      "Epoch[1/15], Step [7800/12500], Reconst Loss: -43556.7383, KL Div: 3631.4666\n",
      "Epoch[1/15], Step [7900/12500], Reconst Loss: -352503.9375, KL Div: 4694.9683\n",
      "Epoch[1/15], Step [8000/12500], Reconst Loss: -109732.2031, KL Div: 3950.9060\n",
      "Epoch[1/15], Step [8100/12500], Reconst Loss: -157794.2500, KL Div: 3681.9341\n",
      "Epoch[1/15], Step [8200/12500], Reconst Loss: 55774.6172, KL Div: 13187.9111\n",
      "Epoch[1/15], Step [8300/12500], Reconst Loss: -174512.0469, KL Div: 13521.0391\n",
      "Epoch[1/15], Step [8400/12500], Reconst Loss: -95346.7031, KL Div: 7523.3125\n",
      "Epoch[1/15], Step [8500/12500], Reconst Loss: 99273.2812, KL Div: 1248777.8750\n",
      "Epoch[1/15], Step [8600/12500], Reconst Loss: -130159.8594, KL Div: 5428.9653\n",
      "Epoch[1/15], Step [8700/12500], Reconst Loss: -169.7516, KL Div: 8629.5664\n",
      "Epoch[1/15], Step [8800/12500], Reconst Loss: -106862.0625, KL Div: 5070.2661\n",
      "Epoch[1/15], Step [8900/12500], Reconst Loss: -128178.4375, KL Div: 2340.1091\n",
      "Epoch[1/15], Step [9000/12500], Reconst Loss: -200815.3438, KL Div: 3595.9839\n",
      "Epoch[1/15], Step [9100/12500], Reconst Loss: -408026.8750, KL Div: 7858.3433\n",
      "Epoch[1/15], Step [9200/12500], Reconst Loss: -133098.0625, KL Div: 3861.4160\n",
      "Epoch[1/15], Step [9300/12500], Reconst Loss: -102468.1484, KL Div: 3115.5293\n",
      "Epoch[1/15], Step [9400/12500], Reconst Loss: -45642.3086, KL Div: 4204.3359\n",
      "Epoch[1/15], Step [9500/12500], Reconst Loss: -41149.6094, KL Div: 4176.4946\n",
      "Epoch[1/15], Step [9600/12500], Reconst Loss: -72588.1875, KL Div: 6009.7065\n",
      "Epoch[1/15], Step [9700/12500], Reconst Loss: -95010.6562, KL Div: 14426.6680\n",
      "Epoch[1/15], Step [9800/12500], Reconst Loss: -215293.7031, KL Div: 6692.0557\n",
      "Epoch[1/15], Step [9900/12500], Reconst Loss: -269543.4688, KL Div: 5585.8267\n",
      "Epoch[1/15], Step [10000/12500], Reconst Loss: -204978.9375, KL Div: 4281.7021\n",
      "Epoch[1/15], Step [10100/12500], Reconst Loss: -39592.1289, KL Div: 8362.5977\n",
      "Epoch[1/15], Step [10200/12500], Reconst Loss: -330570.9375, KL Div: 8164.5547\n",
      "Epoch[1/15], Step [10300/12500], Reconst Loss: -87768.4375, KL Div: 9897.5869\n",
      "Epoch[1/15], Step [10400/12500], Reconst Loss: -134461.5938, KL Div: 3054.7922\n",
      "Epoch[1/15], Step [10500/12500], Reconst Loss: -20216.7266, KL Div: 12134.8721\n",
      "Epoch[1/15], Step [10600/12500], Reconst Loss: -83079.9531, KL Div: 4120.7441\n",
      "Epoch[1/15], Step [10700/12500], Reconst Loss: -26000.3398, KL Div: 878.2535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/15], Step [10800/12500], Reconst Loss: -91991.7812, KL Div: 3575.6445\n",
      "Epoch[1/15], Step [10900/12500], Reconst Loss: -53013.1523, KL Div: 1480.0315\n",
      "Epoch[1/15], Step [11000/12500], Reconst Loss: -106524.1797, KL Div: 4558.2178\n",
      "Epoch[1/15], Step [11100/12500], Reconst Loss: -229987.8594, KL Div: 4747.2598\n",
      "Epoch[1/15], Step [11200/12500], Reconst Loss: -153046.3750, KL Div: 11504.8984\n",
      "Epoch[1/15], Step [11300/12500], Reconst Loss: -29522.7441, KL Div: 6507.3315\n",
      "Epoch[1/15], Step [11400/12500], Reconst Loss: -142111.2500, KL Div: 3627.5979\n",
      "Epoch[1/15], Step [11500/12500], Reconst Loss: -6007.9639, KL Div: 9046.9023\n",
      "Epoch[1/15], Step [11600/12500], Reconst Loss: -359269.0000, KL Div: 5103.2183\n",
      "Epoch[1/15], Step [11700/12500], Reconst Loss: -27736.3496, KL Div: 11327.0400\n",
      "Epoch[1/15], Step [11800/12500], Reconst Loss: -116286.6641, KL Div: 4542.6050\n",
      "Epoch[1/15], Step [11900/12500], Reconst Loss: 7907.1978, KL Div: 7928.8257\n",
      "Epoch[1/15], Step [12000/12500], Reconst Loss: -259339.7344, KL Div: 6923.8730\n",
      "Epoch[1/15], Step [12100/12500], Reconst Loss: -185899.0625, KL Div: 6147.2695\n",
      "Epoch[1/15], Step [12200/12500], Reconst Loss: -152804.6875, KL Div: 1912.1038\n",
      "Epoch[1/15], Step [12300/12500], Reconst Loss: -3721.2075, KL Div: 1205.4878\n",
      "Epoch[1/15], Step [12400/12500], Reconst Loss: -82776.4141, KL Div: 3972.2844\n",
      "Epoch[1/15], Step [12500/12500], Reconst Loss: -121601.0859, KL Div: 3163.2148\n",
      "goingon\n",
      "Epoch[2/15], Step [100/12500], Reconst Loss: -265440.0938, KL Div: 3573.1853\n",
      "Epoch[2/15], Step [200/12500], Reconst Loss: -38201.8438, KL Div: 4507.4312\n",
      "Epoch[2/15], Step [300/12500], Reconst Loss: -296405.8438, KL Div: 7243.2852\n",
      "Epoch[2/15], Step [400/12500], Reconst Loss: -145456.3281, KL Div: 4526.7886\n",
      "Epoch[2/15], Step [500/12500], Reconst Loss: -108190.0547, KL Div: 4770.2021\n",
      "Epoch[2/15], Step [600/12500], Reconst Loss: -56692.7266, KL Div: 3481.2109\n",
      "Epoch[2/15], Step [700/12500], Reconst Loss: 24193.0176, KL Div: 11583.7129\n",
      "Epoch[2/15], Step [800/12500], Reconst Loss: -138988.7812, KL Div: 5173.8911\n",
      "Epoch[2/15], Step [900/12500], Reconst Loss: -80678.3672, KL Div: 5198.3169\n",
      "Epoch[2/15], Step [1000/12500], Reconst Loss: -322165.6875, KL Div: 5157.2471\n",
      "Epoch[2/15], Step [1100/12500], Reconst Loss: -12994.5439, KL Div: 8324.6816\n",
      "Epoch[2/15], Step [1200/12500], Reconst Loss: -114176.1172, KL Div: 2919.2915\n",
      "Epoch[2/15], Step [1300/12500], Reconst Loss: -87402.6797, KL Div: 2790.9722\n",
      "Epoch[2/15], Step [1400/12500], Reconst Loss: -241021.7031, KL Div: 4033.4707\n",
      "Epoch[2/15], Step [1500/12500], Reconst Loss: -82743.2812, KL Div: 3601.5076\n",
      "Epoch[2/15], Step [1600/12500], Reconst Loss: -19158.2617, KL Div: 17628.2227\n",
      "Epoch[2/15], Step [1700/12500], Reconst Loss: -57779.6328, KL Div: 5850.5234\n",
      "Epoch[2/15], Step [1800/12500], Reconst Loss: -129539.3672, KL Div: 4121.7344\n",
      "Epoch[2/15], Step [1900/12500], Reconst Loss: -300904.0938, KL Div: 14005.8398\n",
      "Epoch[2/15], Step [2000/12500], Reconst Loss: -325874.5625, KL Div: 9744.3896\n",
      "Epoch[2/15], Step [2100/12500], Reconst Loss: -71195.0859, KL Div: 4968.4277\n",
      "Epoch[2/15], Step [2200/12500], Reconst Loss: -212580.1406, KL Div: 6797.6787\n",
      "Epoch[2/15], Step [2300/12500], Reconst Loss: -368354.8438, KL Div: 9069.3398\n",
      "Epoch[2/15], Step [2400/12500], Reconst Loss: -897.6238, KL Div: 9059.2783\n",
      "Epoch[2/15], Step [2500/12500], Reconst Loss: -74148.9141, KL Div: 3939.7856\n",
      "Epoch[2/15], Step [2600/12500], Reconst Loss: -60853.2539, KL Div: 6252.5127\n",
      "Epoch[2/15], Step [2700/12500], Reconst Loss: -29016.4805, KL Div: 1893.9512\n",
      "Epoch[2/15], Step [2800/12500], Reconst Loss: -105162.5859, KL Div: 5029.7969\n",
      "Epoch[2/15], Step [2900/12500], Reconst Loss: -50285.4414, KL Div: 2222.9153\n",
      "Epoch[2/15], Step [3000/12500], Reconst Loss: 11065.2588, KL Div: 6561.9404\n",
      "Epoch[2/15], Step [3100/12500], Reconst Loss: -51642.4180, KL Div: 2189.9380\n",
      "Epoch[2/15], Step [3200/12500], Reconst Loss: -50468.3906, KL Div: 3932.8193\n",
      "Epoch[2/15], Step [3300/12500], Reconst Loss: -116297.3047, KL Div: 5312.4854\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-70cf78745fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# 计算重构损失和KL散度（KL散度用于衡量两种分布的相似程度）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# KL散度的计算可以参考论文或者文章开头的链接\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mreconst_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_reconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mkl_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2891\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2893\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"goingon\")\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # 获取样本，并前向传播\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        # 计算重构损失和KL散度（KL散度用于衡量两种分布的相似程度）\n",
    "        # KL散度的计算可以参考论文或者文章开头的链接\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    \n",
    "    # 利用训练的模型进行测试\n",
    "    with torch.no_grad():\n",
    "        # 随机生成的图像\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 96, 128)\n",
    "        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "        # 重构的图像\n",
    "        out, _, _ = model(x)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 96, 128), out.view(-1, 1,96, 128)], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
