{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download the data. Implement a data loader class to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x:x.repeat(3,1,1)),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])#When the dataset is loaded, the default image format is Numpy, so transforms it into a Tensor via transforms.\n",
    "# Then, the input image is normalized.\n",
    "# After Normalize, perform the following operations on each channel: image = (image - average) /std\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img*0.5 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))    #The tensor tensor in PyTorch stores the image as (b, C,w,h) (number of images, number of channels, height, width).\n",
    "\n",
    "                                                    #In tensor, the number of pipes, width, height.And the standard RBG image is (width, height, number of pipes).\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "(data,label)=trainset[28]\n",
    "print(classes[label])\n",
    "imshow(data)\n",
    "print(label)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)   \n",
    "images, labels = dataiter.next()  \n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)             #define convolutional layers and pooling layers\n",
    "        self.conv2 = nn.Conv2d(24, 48, 5)\n",
    "        self.conv3 = nn.Conv2d(48, 64, 3)\n",
    "        self.fc1 = nn.Linear(64, 120)       #define fully-connected layers\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Utilize a gradient-based optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()         #用交叉熵作loss function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  #随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.649\n",
      "[1,  4000] loss: 0.241\n",
      "[1,  6000] loss: 0.147\n",
      "[1,  8000] loss: 0.116\n",
      "[1, 10000] loss: 0.095\n",
      "[1, 12000] loss: 0.079\n",
      "[1, 14000] loss: 0.075\n",
      "[2,  2000] loss: 0.061\n",
      "[2,  4000] loss: 0.061\n",
      "[2,  6000] loss: 0.058\n",
      "[2,  8000] loss: 0.053\n",
      "[2, 10000] loss: 0.051\n",
      "[2, 12000] loss: 0.050\n",
      "[2, 14000] loss: 0.043\n",
      "[3,  2000] loss: 0.035\n",
      "[3,  4000] loss: 0.032\n",
      "[3,  6000] loss: 0.037\n",
      "[3,  8000] loss: 0.036\n",
      "[3, 10000] loss: 0.042\n",
      "[3, 12000] loss: 0.034\n",
      "[3, 14000] loss: 0.044\n",
      "[4,  2000] loss: 0.024\n",
      "[4,  4000] loss: 0.026\n",
      "[4,  6000] loss: 0.024\n",
      "[4,  8000] loss: 0.032\n",
      "[4, 10000] loss: 0.032\n",
      "[4, 12000] loss: 0.026\n",
      "[4, 14000] loss: 0.029\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data          \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()       #gradient starts from 0\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the trained model on an independent test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:      7     2     1     0\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:      7     2     1     0\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)        #put it into model\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()  \n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0     is: 99.0 %\n",
      "Accuracy for class 1     is: 99.9 %\n",
      "Accuracy for class 2     is: 99.3 %\n",
      "Accuracy for class 3     is: 99.6 %\n",
      "Accuracy for class 4     is: 99.3 %\n",
      "Accuracy for class 5     is: 98.7 %\n",
      "Accuracy for class 6     is: 99.1 %\n",
      "Accuracy for class 7     is: 98.7 %\n",
      "Accuracy for class 8     is: 99.1 %\n",
      "Accuracy for class 9     is: 99.0 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}  #define list\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "D:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.510\n",
      "[1,  4000] loss: 0.232\n",
      "[1,  6000] loss: 0.149\n",
      "[1,  8000] loss: 0.104\n",
      "[1, 10000] loss: 0.094\n",
      "[1, 12000] loss: 0.084\n",
      "[1, 14000] loss: 0.071\n",
      "Accuracy of the network on the 2500 test images: 97 %\n",
      "[1] loss: 0.063\n",
      "14999\n",
      "[2,  2000] loss: 0.058\n",
      "[2,  4000] loss: 0.055\n",
      "[2,  6000] loss: 0.062\n",
      "[2,  8000] loss: 0.061\n",
      "[2, 10000] loss: 0.051\n",
      "[2, 12000] loss: 0.051\n",
      "[2, 14000] loss: 0.048\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[2] loss: 0.061\n",
      "14999\n",
      "[3,  2000] loss: 0.040\n",
      "[3,  4000] loss: 0.041\n",
      "[3,  6000] loss: 0.037\n",
      "[3,  8000] loss: 0.041\n",
      "[3, 10000] loss: 0.039\n",
      "[3, 12000] loss: 0.033\n",
      "[3, 14000] loss: 0.033\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[3] loss: 0.037\n",
      "14999\n",
      "[4,  2000] loss: 0.029\n",
      "[4,  4000] loss: 0.025\n",
      "[4,  6000] loss: 0.026\n",
      "[4,  8000] loss: 0.022\n",
      "[4, 10000] loss: 0.034\n",
      "[4, 12000] loss: 0.027\n",
      "[4, 14000] loss: 0.034\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[4] loss: 0.055\n",
      "14999\n",
      "[5,  2000] loss: 0.019\n",
      "[5,  4000] loss: 0.021\n",
      "[5,  6000] loss: 0.023\n",
      "[5,  8000] loss: 0.022\n",
      "[5, 10000] loss: 0.026\n",
      "[5, 12000] loss: 0.027\n",
      "[5, 14000] loss: 0.020\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[5] loss: 0.032\n",
      "14999\n",
      "[6,  2000] loss: 0.017\n",
      "[6,  4000] loss: 0.012\n",
      "[6,  6000] loss: 0.019\n",
      "[6,  8000] loss: 0.017\n",
      "[6, 10000] loss: 0.020\n",
      "[6, 12000] loss: 0.017\n",
      "[6, 14000] loss: 0.026\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[6] loss: 0.029\n",
      "14999\n",
      "[7,  2000] loss: 0.012\n",
      "[7,  4000] loss: 0.016\n",
      "[7,  6000] loss: 0.011\n",
      "[7,  8000] loss: 0.018\n",
      "[7, 10000] loss: 0.014\n",
      "[7, 12000] loss: 0.016\n",
      "[7, 14000] loss: 0.016\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[7] loss: 0.033\n",
      "14999\n",
      "[8,  2000] loss: 0.013\n",
      "[8,  4000] loss: 0.011\n",
      "[8,  6000] loss: 0.012\n",
      "[8,  8000] loss: 0.008\n",
      "[8, 10000] loss: 0.016\n",
      "[8, 12000] loss: 0.016\n",
      "[8, 14000] loss: 0.010\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[8] loss: 0.035\n",
      "14999\n",
      "[9,  2000] loss: 0.007\n",
      "[9,  4000] loss: 0.010\n",
      "[9,  6000] loss: 0.015\n",
      "[9,  8000] loss: 0.009\n",
      "[9, 10000] loss: 0.007\n",
      "[9, 12000] loss: 0.012\n",
      "[9, 14000] loss: 0.009\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[9] loss: 0.050\n",
      "14999\n",
      "[10,  2000] loss: 0.008\n",
      "[10,  4000] loss: 0.011\n",
      "[10,  6000] loss: 0.008\n",
      "[10,  8000] loss: 0.012\n",
      "[10, 10000] loss: 0.015\n",
      "[10, 12000] loss: 0.012\n",
      "[10, 14000] loss: 0.019\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[10] loss: 0.032\n",
      "14999\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "visdom_show = Visdom(env=\"trainloss\")\n",
    "visdom_accu = Visdom(env=\"testaccu\")\n",
    "visdom_test = Visdom(env=\"testloss\")\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "        #visualization\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data          \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()       #start with zero\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            if (i+1)==12000:\n",
    "                visdom_show.line(\n",
    "                    X=[epoch+1],\n",
    "                    Y=[float(running_loss / 2000)],\n",
    "                    win='accu and loss',\n",
    "                    name='train_loss',\n",
    "                    opts=dict(title='accu and loss',  legend=['train_loss']),\n",
    "                    update='append')\n",
    "            running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        running_loss2=0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)        \n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss2 += loss.item()\n",
    "        print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total))\n",
    "        visdom_accu.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[(correct / total)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_accu']),\n",
    "            name='test_accu',\n",
    "            update='append')\n",
    "        print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1,running_loss2 / 2500))\n",
    "        visdom_test.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[float(running_loss2 / 2500)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_loss']),\n",
    "            name='test_loss',\n",
    "            update='append')\n",
    "        print(i)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Provide a summary of any architectural modifications made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a summary of any architectural modifications made, plots of the loss evolution from (4), and plots of your results from (5). Please save your code and results for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I try to change the convlutional layers to 4, and the activation function to the variation of relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)             #define convolutional layers and pooling layers\n",
    "        self.conv2 = nn.Conv2d(12, 24, 2)\n",
    "        self.conv3 = nn.Conv2d(24, 48, 2)\n",
    "        self.conv4 = nn.Conv2d(48, 64, 1)\n",
    "        self.fc1 = nn.Linear(64, 120)       #define fully-connected layers\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.rrelu(self.conv1(x)))\n",
    "        x = self.pool(F.rrelu(self.conv2(x)))\n",
    "        x = self.pool(F.rrelu(self.conv3(x)))\n",
    "        x = self.pool(F.rrelu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.rrelu(self.fc1(x))\n",
    "        x = F.rrelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()         #用交叉熵作loss function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  #随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.299\n",
      "[1,  4000] loss: 2.044\n",
      "[1,  6000] loss: 0.910\n",
      "[1,  8000] loss: 0.344\n",
      "[1, 10000] loss: 0.237\n",
      "[1, 12000] loss: 0.159\n",
      "[1, 14000] loss: 0.143\n",
      "Accuracy of the network on the 2500 test images: 96 %\n",
      "[1] loss: 0.117\n",
      "14999\n",
      "[2,  2000] loss: 0.124\n",
      "[2,  4000] loss: 0.095\n",
      "[2,  6000] loss: 0.096\n",
      "[2,  8000] loss: 0.109\n",
      "[2, 10000] loss: 0.081\n",
      "[2, 12000] loss: 0.091\n",
      "[2, 14000] loss: 0.080\n",
      "Accuracy of the network on the 2500 test images: 97 %\n",
      "[2] loss: 0.068\n",
      "14999\n",
      "[3,  2000] loss: 0.071\n",
      "[3,  4000] loss: 0.069\n",
      "[3,  6000] loss: 0.068\n",
      "[3,  8000] loss: 0.068\n",
      "[3, 10000] loss: 0.059\n",
      "[3, 12000] loss: 0.063\n",
      "[3, 14000] loss: 0.070\n",
      "Accuracy of the network on the 2500 test images: 97 %\n",
      "[3] loss: 0.068\n",
      "14999\n",
      "[4,  2000] loss: 0.057\n",
      "[4,  4000] loss: 0.051\n",
      "[4,  6000] loss: 0.050\n",
      "[4,  8000] loss: 0.050\n",
      "[4, 10000] loss: 0.054\n",
      "[4, 12000] loss: 0.051\n",
      "[4, 14000] loss: 0.050\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[4] loss: 0.049\n",
      "14999\n",
      "[5,  2000] loss: 0.033\n",
      "[5,  4000] loss: 0.039\n",
      "[5,  6000] loss: 0.052\n",
      "[5,  8000] loss: 0.045\n",
      "[5, 10000] loss: 0.039\n",
      "[5, 12000] loss: 0.043\n",
      "[5, 14000] loss: 0.053\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[5] loss: 0.050\n",
      "14999\n",
      "[6,  2000] loss: 0.034\n",
      "[6,  4000] loss: 0.044\n",
      "[6,  6000] loss: 0.043\n",
      "[6,  8000] loss: 0.033\n",
      "[6, 10000] loss: 0.032\n",
      "[6, 12000] loss: 0.046\n",
      "[6, 14000] loss: 0.037\n",
      "Accuracy of the network on the 2500 test images: 97 %\n",
      "[6] loss: 0.087\n",
      "14999\n",
      "[7,  2000] loss: 0.030\n",
      "[7,  4000] loss: 0.035\n",
      "[7,  6000] loss: 0.034\n",
      "[7,  8000] loss: 0.026\n",
      "[7, 10000] loss: 0.036\n",
      "[7, 12000] loss: 0.035\n",
      "[7, 14000] loss: 0.034\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[7] loss: 0.041\n",
      "14999\n",
      "[8,  2000] loss: 0.027\n",
      "[8,  4000] loss: 0.026\n",
      "[8,  6000] loss: 0.029\n",
      "[8,  8000] loss: 0.031\n",
      "[8, 10000] loss: 0.037\n",
      "[8, 12000] loss: 0.029\n",
      "[8, 14000] loss: 0.029\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[8] loss: 0.033\n",
      "14999\n",
      "[9,  2000] loss: 0.022\n",
      "[9,  4000] loss: 0.026\n",
      "[9,  6000] loss: 0.023\n",
      "[9,  8000] loss: 0.033\n",
      "[9, 10000] loss: 0.026\n",
      "[9, 12000] loss: 0.024\n",
      "[9, 14000] loss: 0.028\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[9] loss: 0.039\n",
      "14999\n",
      "[10,  2000] loss: 0.023\n",
      "[10,  4000] loss: 0.019\n",
      "[10,  6000] loss: 0.024\n",
      "[10,  8000] loss: 0.024\n",
      "[10, 10000] loss: 0.025\n",
      "[10, 12000] loss: 0.027\n",
      "[10, 14000] loss: 0.031\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[10] loss: 0.039\n",
      "14999\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "visdom_show = Visdom(env=\"trainloss_new\")\n",
    "visdom_accu = Visdom(env=\"testaccu_new\")\n",
    "visdom_test = Visdom(env=\"testloss_new\")\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "        #可视化\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data          \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()       #梯度从零开始\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            if (i+1)==12000:\n",
    "                visdom_show.line(\n",
    "                    X=[epoch+1],\n",
    "                    Y=[float(running_loss / 2000)],\n",
    "                    win='accu and loss',\n",
    "                    name='train_loss_new',\n",
    "                    opts=dict(title='accu and loss',  legend=['train_loss_new']),\n",
    "                    update='append')\n",
    "            running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        running_loss2=0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)        \n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss2 += loss.item()\n",
    "        print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total))\n",
    "        visdom_accu.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[(correct / total)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_accu_new']),\n",
    "            name='test_accu_new',\n",
    "            update='append')\n",
    "        print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1,running_loss2 / 2500))\n",
    "        visdom_test.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[float(running_loss2 / 2500)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_loss_new']),\n",
    "            name='test_loss_new',\n",
    "            update='append')\n",
    "        print(i)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I try to compare diffrent relu function as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x:x.repeat(3,1,1)),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])#When the dataset is loaded, the default image format is Numpy, so transforms it into a Tensor via transforms.\n",
    "# Then, the input image is normalized.\n",
    "# After Normalize, perform the following operations on each channel: image = (image - average) /std\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)             #define convolutional layers and pooling layers\n",
    "        self.conv2 = nn.Conv2d(24, 48, 5)\n",
    "        self.conv3 = nn.Conv2d(48, 64, 3)\n",
    "        self.fc1 = nn.Linear(64, 120)       #define fully-connected layers\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = Net()\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()         #用交叉熵作loss function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  #随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "D:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.746\n",
      "[1,  4000] loss: 0.261\n",
      "[1,  6000] loss: 0.141\n",
      "[1,  8000] loss: 0.113\n",
      "[1, 10000] loss: 0.102\n",
      "[1, 12000] loss: 0.074\n",
      "[1, 14000] loss: 0.086\n",
      "Accuracy of the network on the 2500 test images: 97 %\n",
      "[1] loss: 0.062\n",
      "14999\n",
      "[2,  2000] loss: 0.058\n",
      "[2,  4000] loss: 0.053\n",
      "[2,  6000] loss: 0.055\n",
      "[2,  8000] loss: 0.058\n",
      "[2, 10000] loss: 0.052\n",
      "[2, 12000] loss: 0.050\n",
      "[2, 14000] loss: 0.055\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[2] loss: 0.039\n",
      "14999\n",
      "[3,  2000] loss: 0.032\n",
      "[3,  4000] loss: 0.038\n",
      "[3,  6000] loss: 0.039\n",
      "[3,  8000] loss: 0.036\n",
      "[3, 10000] loss: 0.041\n",
      "[3, 12000] loss: 0.034\n",
      "[3, 14000] loss: 0.041\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[3] loss: 0.034\n",
      "14999\n",
      "[4,  2000] loss: 0.025\n",
      "[4,  4000] loss: 0.034\n",
      "[4,  6000] loss: 0.026\n",
      "[4,  8000] loss: 0.033\n",
      "[4, 10000] loss: 0.025\n",
      "[4, 12000] loss: 0.026\n",
      "[4, 14000] loss: 0.036\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[4] loss: 0.034\n",
      "14999\n",
      "[5,  2000] loss: 0.021\n",
      "[5,  4000] loss: 0.023\n",
      "[5,  6000] loss: 0.021\n",
      "[5,  8000] loss: 0.014\n",
      "[5, 10000] loss: 0.027\n",
      "[5, 12000] loss: 0.023\n",
      "[5, 14000] loss: 0.024\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[5] loss: 0.033\n",
      "14999\n",
      "[6,  2000] loss: 0.012\n",
      "[6,  4000] loss: 0.019\n",
      "[6,  6000] loss: 0.021\n",
      "[6,  8000] loss: 0.021\n",
      "[6, 10000] loss: 0.018\n",
      "[6, 12000] loss: 0.019\n",
      "[6, 14000] loss: 0.019\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[6] loss: 0.033\n",
      "14999\n",
      "[7,  2000] loss: 0.017\n",
      "[7,  4000] loss: 0.015\n",
      "[7,  6000] loss: 0.017\n",
      "[7,  8000] loss: 0.020\n",
      "[7, 10000] loss: 0.015\n",
      "[7, 12000] loss: 0.013\n",
      "[7, 14000] loss: 0.014\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[7] loss: 0.026\n",
      "14999\n",
      "[8,  2000] loss: 0.012\n",
      "[8,  4000] loss: 0.013\n",
      "[8,  6000] loss: 0.008\n",
      "[8,  8000] loss: 0.014\n",
      "[8, 10000] loss: 0.015\n",
      "[8, 12000] loss: 0.013\n",
      "[8, 14000] loss: 0.015\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[8] loss: 0.032\n",
      "14999\n",
      "[9,  2000] loss: 0.008\n",
      "[9,  4000] loss: 0.010\n",
      "[9,  6000] loss: 0.013\n",
      "[9,  8000] loss: 0.015\n",
      "[9, 10000] loss: 0.012\n",
      "[9, 12000] loss: 0.009\n",
      "[9, 14000] loss: 0.017\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[9] loss: 0.063\n",
      "14999\n",
      "[10,  2000] loss: 0.006\n",
      "[10,  4000] loss: 0.009\n",
      "[10,  6000] loss: 0.008\n",
      "[10,  8000] loss: 0.006\n",
      "[10, 10000] loss: 0.007\n",
      "[10, 12000] loss: 0.007\n",
      "[10, 14000] loss: 0.014\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[10] loss: 0.042\n",
      "14999\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "visdom_show = Visdom(env=\"trainloss_relu\")\n",
    "visdom_accu = Visdom(env=\"testaccu_relu\")\n",
    "visdom_test = Visdom(env=\"testloss_relu\")\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "        #可视化\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data          \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()       #梯度从零开始\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            if (i+1)==12000:\n",
    "                visdom_show.line(\n",
    "                    X=[epoch+1],\n",
    "                    Y=[float(running_loss / 2000)],\n",
    "                    win='accu and loss',\n",
    "                    name='train_loss_relu',\n",
    "                    opts=dict(title='accu and loss',  legend=['train_loss_relu']),\n",
    "                    update='append')\n",
    "            running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        running_loss2=0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)        #放进模型里\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  #相同的累加\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss2 += loss.item()\n",
    "        print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total))\n",
    "        visdom_accu.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[(correct / total)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_accu_relu']),\n",
    "            name='test_accu_relu',\n",
    "            update='append')\n",
    "        print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1,running_loss2 / 2500))\n",
    "        visdom_test.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[float(running_loss2 / 2500)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_loss_relu']),\n",
    "            name='test_loss_relu',\n",
    "            update='append')\n",
    "        print(i)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x:x.repeat(3,1,1)),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])#When the dataset is loaded, the default image format is Numpy, so transforms it into a Tensor via transforms.\n",
    "# Then, the input image is normalized.\n",
    "# After Normalize, perform the following operations on each channel: image = (image - average) /std\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)             #define convolutional layers and pooling layers\n",
    "        self.conv2 = nn.Conv2d(24, 48, 5)\n",
    "        self.conv3 = nn.Conv2d(48, 64, 3)\n",
    "        self.fc1 = nn.Linear(64, 120)       #define fully-connected layers\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = Net()\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()         #用交叉熵作loss function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  #随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.561\n",
      "[1,  4000] loss: 0.235\n",
      "[1,  6000] loss: 0.142\n",
      "[1,  8000] loss: 0.116\n",
      "[1, 10000] loss: 0.094\n",
      "[1, 12000] loss: 0.090\n",
      "[1, 14000] loss: 0.075\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[1] loss: 0.053\n",
      "14999\n",
      "[2,  2000] loss: 0.062\n",
      "[2,  4000] loss: 0.051\n",
      "[2,  6000] loss: 0.058\n",
      "[2,  8000] loss: 0.052\n",
      "[2, 10000] loss: 0.055\n",
      "[2, 12000] loss: 0.052\n",
      "[2, 14000] loss: 0.056\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[2] loss: 0.044\n",
      "14999\n",
      "[3,  2000] loss: 0.038\n",
      "[3,  4000] loss: 0.038\n",
      "[3,  6000] loss: 0.036\n",
      "[3,  8000] loss: 0.044\n",
      "[3, 10000] loss: 0.041\n",
      "[3, 12000] loss: 0.037\n",
      "[3, 14000] loss: 0.035\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[3] loss: 0.034\n",
      "14999\n",
      "[4,  2000] loss: 0.031\n",
      "[4,  4000] loss: 0.032\n",
      "[4,  6000] loss: 0.028\n",
      "[4,  8000] loss: 0.032\n",
      "[4, 10000] loss: 0.026\n",
      "[4, 12000] loss: 0.029\n",
      "[4, 14000] loss: 0.024\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[4] loss: 0.042\n",
      "14999\n",
      "[5,  2000] loss: 0.022\n",
      "[5,  4000] loss: 0.025\n",
      "[5,  6000] loss: 0.021\n",
      "[5,  8000] loss: 0.024\n",
      "[5, 10000] loss: 0.023\n",
      "[5, 12000] loss: 0.024\n",
      "[5, 14000] loss: 0.021\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[5] loss: 0.029\n",
      "14999\n",
      "[6,  2000] loss: 0.014\n",
      "[6,  4000] loss: 0.016\n",
      "[6,  6000] loss: 0.020\n",
      "[6,  8000] loss: 0.022\n",
      "[6, 10000] loss: 0.022\n",
      "[6, 12000] loss: 0.016\n",
      "[6, 14000] loss: 0.024\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[6] loss: 0.030\n",
      "14999\n",
      "[7,  2000] loss: 0.012\n",
      "[7,  4000] loss: 0.010\n",
      "[7,  6000] loss: 0.011\n",
      "[7,  8000] loss: 0.016\n",
      "[7, 10000] loss: 0.022\n",
      "[7, 12000] loss: 0.019\n",
      "[7, 14000] loss: 0.019\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[7] loss: 0.029\n",
      "14999\n",
      "[8,  2000] loss: 0.009\n",
      "[8,  4000] loss: 0.013\n",
      "[8,  6000] loss: 0.016\n",
      "[8,  8000] loss: 0.010\n",
      "[8, 10000] loss: 0.007\n",
      "[8, 12000] loss: 0.013\n",
      "[8, 14000] loss: 0.015\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[8] loss: 0.034\n",
      "14999\n",
      "[9,  2000] loss: 0.011\n",
      "[9,  4000] loss: 0.009\n",
      "[9,  6000] loss: 0.013\n",
      "[9,  8000] loss: 0.009\n",
      "[9, 10000] loss: 0.009\n",
      "[9, 12000] loss: 0.010\n",
      "[9, 14000] loss: 0.010\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[9] loss: 0.037\n",
      "14999\n",
      "[10,  2000] loss: 0.011\n",
      "[10,  4000] loss: 0.008\n",
      "[10,  6000] loss: 0.008\n",
      "[10,  8000] loss: 0.011\n",
      "[10, 10000] loss: 0.009\n",
      "[10, 12000] loss: 0.013\n",
      "[10, 14000] loss: 0.012\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[10] loss: 0.027\n",
      "14999\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "visdom_show = Visdom(env=\"trainloss_leakyrelu\")\n",
    "visdom_accu = Visdom(env=\"testaccu_leakyrelu\")\n",
    "visdom_test = Visdom(env=\"testloss_leakyrelu\")\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "        #可视化\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data          \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()       #梯度从零开始\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            if (i+1)==12000:\n",
    "                visdom_show.line(\n",
    "                    X=[epoch+1],\n",
    "                    Y=[float(running_loss / 2000)],\n",
    "                    win='accu and loss',\n",
    "                    name='train_loss_leakyrelu',\n",
    "                    opts=dict(title='accu and loss',  legend=['train_loss_leakyrelu']),\n",
    "                    update='append')\n",
    "            running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        running_loss2=0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)        #放进模型里\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  #相同的累加\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss2 += loss.item()\n",
    "        print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total))\n",
    "        visdom_accu.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[(correct / total)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_accu_leakyrelu']),\n",
    "            name='test_accu_leakyrelu',\n",
    "            update='append')\n",
    "        print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1,running_loss2 / 2500))\n",
    "        visdom_test.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[float(running_loss2 / 2500)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_loss_leakyrelu']),\n",
    "            name='test_loss_leakyrelu',\n",
    "            update='append')\n",
    "        print(i)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x:x.repeat(3,1,1)),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])#When the dataset is loaded, the default image format is Numpy, so transforms it into a Tensor via transforms.\n",
    "# Then, the input image is normalized.\n",
    "# After Normalize, perform the following operations on each channel: image = (image - average) /std\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)             #define convolutional layers and pooling layers\n",
    "        self.conv2 = nn.Conv2d(24, 48, 5)\n",
    "        self.conv3 = nn.Conv2d(48, 64, 3)\n",
    "        self.fc1 = nn.Linear(64, 120)       #define fully-connected layers\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = Net()\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()         #用交叉熵作loss function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  #随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.520\n",
      "[1,  4000] loss: 0.225\n",
      "[1,  6000] loss: 0.125\n",
      "[1,  8000] loss: 0.105\n",
      "[1, 10000] loss: 0.088\n",
      "[1, 12000] loss: 0.078\n",
      "[1, 14000] loss: 0.078\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[1] loss: 0.044\n",
      "14999\n",
      "[2,  2000] loss: 0.057\n",
      "[2,  4000] loss: 0.054\n",
      "[2,  6000] loss: 0.054\n",
      "[2,  8000] loss: 0.053\n",
      "[2, 10000] loss: 0.041\n",
      "[2, 12000] loss: 0.053\n",
      "[2, 14000] loss: 0.050\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[2] loss: 0.037\n",
      "14999\n",
      "[3,  2000] loss: 0.041\n",
      "[3,  4000] loss: 0.037\n",
      "[3,  6000] loss: 0.032\n",
      "[3,  8000] loss: 0.036\n",
      "[3, 10000] loss: 0.042\n",
      "[3, 12000] loss: 0.032\n",
      "[3, 14000] loss: 0.033\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[3] loss: 0.039\n",
      "14999\n",
      "[4,  2000] loss: 0.029\n",
      "[4,  4000] loss: 0.033\n",
      "[4,  6000] loss: 0.023\n",
      "[4,  8000] loss: 0.031\n",
      "[4, 10000] loss: 0.027\n",
      "[4, 12000] loss: 0.025\n",
      "[4, 14000] loss: 0.033\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[4] loss: 0.029\n",
      "14999\n",
      "[5,  2000] loss: 0.021\n",
      "[5,  4000] loss: 0.016\n",
      "[5,  6000] loss: 0.020\n",
      "[5,  8000] loss: 0.026\n",
      "[5, 10000] loss: 0.024\n",
      "[5, 12000] loss: 0.022\n",
      "[5, 14000] loss: 0.022\n",
      "Accuracy of the network on the 2500 test images: 98 %\n",
      "[5] loss: 0.033\n",
      "14999\n",
      "[6,  2000] loss: 0.015\n",
      "[6,  4000] loss: 0.019\n",
      "[6,  6000] loss: 0.018\n",
      "[6,  8000] loss: 0.018\n",
      "[6, 10000] loss: 0.020\n",
      "[6, 12000] loss: 0.020\n",
      "[6, 14000] loss: 0.017\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[6] loss: 0.030\n",
      "14999\n",
      "[7,  2000] loss: 0.010\n",
      "[7,  4000] loss: 0.014\n",
      "[7,  6000] loss: 0.015\n",
      "[7,  8000] loss: 0.015\n",
      "[7, 10000] loss: 0.015\n",
      "[7, 12000] loss: 0.018\n",
      "[7, 14000] loss: 0.019\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[7] loss: 0.033\n",
      "14999\n",
      "[8,  2000] loss: 0.012\n",
      "[8,  4000] loss: 0.013\n",
      "[8,  6000] loss: 0.016\n",
      "[8,  8000] loss: 0.009\n",
      "[8, 10000] loss: 0.010\n",
      "[8, 12000] loss: 0.013\n",
      "[8, 14000] loss: 0.016\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[8] loss: 0.028\n",
      "14999\n",
      "[9,  2000] loss: 0.010\n",
      "[9,  4000] loss: 0.009\n",
      "[9,  6000] loss: 0.013\n",
      "[9,  8000] loss: 0.013\n",
      "[9, 10000] loss: 0.014\n",
      "[9, 12000] loss: 0.012\n",
      "[9, 14000] loss: 0.010\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[9] loss: 0.028\n",
      "14999\n",
      "[10,  2000] loss: 0.011\n",
      "[10,  4000] loss: 0.005\n",
      "[10,  6000] loss: 0.011\n",
      "[10,  8000] loss: 0.006\n",
      "[10, 10000] loss: 0.007\n",
      "[10, 12000] loss: 0.008\n",
      "[10, 14000] loss: 0.016\n",
      "Accuracy of the network on the 2500 test images: 99 %\n",
      "[10] loss: 0.031\n",
      "14999\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "visdom_show = Visdom(env=\"trainloss_rrelu\")\n",
    "visdom_accu = Visdom(env=\"testaccu_rrelu\")\n",
    "visdom_test = Visdom(env=\"testloss_rrelu\")\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "        #可视化\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data          \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()       #梯度从零开始\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            if (i+1)==12000:\n",
    "                visdom_show.line(\n",
    "                    X=[epoch+1],\n",
    "                    Y=[float(running_loss / 2000)],\n",
    "                    win='accu and loss',\n",
    "                    name='train_loss_rrelu',\n",
    "                    opts=dict(title='accu and loss',  legend=['train_loss_rrelu']),\n",
    "                    update='append')\n",
    "            running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        running_loss2=0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)        #放进模型里\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  #相同的累加\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss2 += loss.item()\n",
    "        print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total))\n",
    "        visdom_accu.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[(correct / total)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_accu_rrelu']),\n",
    "            name='test_accu_rrelu',\n",
    "            update='append')\n",
    "        print('[%d] loss: %.3f' %\n",
    "                  (epoch + 1,running_loss2 / 2500))\n",
    "        visdom_test.line(\n",
    "            X=[epoch+1],\n",
    "            Y=[float(running_loss2 / 2500)],\n",
    "            win='accu and loss',\n",
    "            opts=dict(title='accu and loss',  legend=['test_loss_rrelu']),\n",
    "            name='test_loss_rrelu',\n",
    "            update='append')\n",
    "        print(i)\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
